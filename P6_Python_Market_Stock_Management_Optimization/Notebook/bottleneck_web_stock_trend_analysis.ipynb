{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"display: flex; background-color: RGB(255,114,0);\" >\n",
    " <h1 style=\"margin: auto; padding: 30px; \">ANALYSE DU STOCK ET DES VENTES DU SITE BOTTLENECK</h1>\n",
    " </div>\n",
    "\n",
    " # OBJECTIF DE CE NOTEBOOK<br>\n",
    " Bienvenue dans l'outil plébiscité par les analystes de données Jupyter.<br>\n",
    " Il s'agit d'un outil permettant de mixer et d'alterner codes, textes et graphique.<br>\n",
    " Cet outil est formidable pour plusieurs raisons:\n",
    " + il permet de tester des lignes de codes au fur et à mesure de votre rédaction, de constater immédiatement le résultat d'un instruction, de la corriger si nécessaire.\n",
    " + De rédiger du texte pour expliquer l'approche suivie ou les résultats d'une analyse et de le mettre en forme grâce à du code html ou plus simple avec **Markdown**\n",
    " + d'agrémenter de graphiques<br>\n",
    " Pour vous aider dans vos premiers pas à l'usage de Jupyter et de Python, nous avons rédigé ce notebook en vous indiquant les instructions à suivre.<br>\n",
    " Il vous suffit pour cela de saisir le code Python répondant à l'instruction donnée.<br>\n",
    " Vous verrez de temps à autre le code Python répondant à une instruction donnée mais cela est fait pour vous aider à comprendre la nature du travail qui vous est demandée.<br>\n",
    " Et garder à l'esprit, qu'il n'y a pas de solution unique pour résoudre un problème et qu'il y a autant de résolutions de problèmes que de développeurs ;)... <br>\n",
    "\n",
    " <div style=\"background-color: RGB(51,165,182);\" >\n",
    " <h2 style=\"margin: auto; padding: 20px; color:#fff; \">Etape 1 - Importation des librairies et chargement des fichiers</h2>\n",
    " </div><br>\n",
    "\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">1.1 - Importation des librairies</h3>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Importation de la librairie Pandas (manipulation de tables)\n",
    "# Librairies de calcul sur tables :\n",
    "import math\n",
    "import numpy as np\n",
    "# Librairies de création et visualisation graphiques :\n",
    "import plotly.express as px #Importation de la librairie plotly express\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "# Librairies annexes :\n",
    "import chardet # Librairie d'encodage csv\n",
    "import warnings; warnings.filterwarnings('ignore') # Librairie de cache warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amélioration de lecture :\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "separe = '-'*75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">1.2 - Initialisation du projet</h3>\n",
    " </div>\n",
    "\n",
    " #### 1.1.2 Automatisation de l'analyse exploratoire et de recherche d'encodage et séparateur d'une feuille de donnée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  La fonction d'analyse exploratoire de fichiers prend en entrée quatre arguments :\n",
    "#  - get_table_info(nom_df, nom_csv)\n",
    "def get_table_info(df, table_name):\n",
    "    text = (f\"Le tableau {table_name} comporte {df.shape[1]} colonne(s) et {df.shape[0]} observation(s) ou article(s).\\n\") # Affichage des dimensions de la table\n",
    "    print(f\"Voici les 5 premières lignes du tableau {table_name}:\") # Affichage des 5 premières lignes du DataFrame\n",
    "    display(df.head())\n",
    "    print(f\"Voici les informations du tableau {table_name}:\") # Affichage des informations sur le DataFrame\n",
    "    display(df.info())\n",
    "    return text\n",
    "\n",
    "#  - analyse_dataframe(nom_df, nom_csv, optionnellement: nombre_n_colonnes, detail_column_unique)\n",
    "def analyze_dataframe(df, table_name, columns=None, distinct=None):\n",
    "# Formatage d'une string avec opérateur ternaire via lambda afin de vérifier et compter les lignes dupliquées et les cellules vides \n",
    "    get_info = lambda duplicated_rows, null_cells: (f\"il se trouve {duplicated_rows} lignes dupliquées, \" if duplicated_rows > 0 else \"aucune ligne n'est dupliquée, \") + (f\"et il se trouve {null_cells} cellules nulles.\\n\" if null_cells > 0 else \"aucune cellule n'est vide.\\n\")\n",
    "    text = f\"Dans la table {table_name}, {get_info(sum(df.duplicated()), sum(df.isnull().any()))}\"\n",
    "# L'argument columns effectue une itération sur chaque colonne spécifié si l'argument est appelé\n",
    "    if columns is not None:\n",
    "        for column in columns:\n",
    "            duplicated_values = sum(df.duplicated(column))\n",
    "            non_null_duplicated_values = sum(df.duplicated(column) & df[column].notnull())\n",
    "            null_values = df[column].isnull().sum()\n",
    "# Formatage d'une string avec opérateur ternaire via lambda afin de vérifier et compter les valeurs dupliquées et/ou nulles. \n",
    "            text += f\"Dans la colonne {column}, \"\n",
    "            text += f\"il se trouve {duplicated_values} valeurs dupliquées, dont {non_null_duplicated_values} valeurs non-nulles \" if duplicated_values > 0 else  f\"il ne se trouve pas de valeur dupliquée. Le nombre d'articles correspond au nombre de lignes.\\n\"\n",
    "            text += f\"et {null_values} valeurs nulles.\\nIl s'y trouve donc {df[column].nunique()} valeurs distinctes sur {df.shape[0]} lignes.\\n\" if null_values > 0 and duplicated_values > 0 else \"\"\n",
    "\n",
    "    if distinct is not None:\n",
    "        unique_values = df[distinct].unique()\n",
    "        unique_values_str = list(map(str, unique_values))\n",
    "        unique_values_text = ', '.join(unique_values_str)\n",
    "        text += f\"Dans la colonne {distinct}, il se trouve {df[distinct].nunique()} valeurs distinctes, que sont : {unique_values_text}.\\n\"\n",
    "    \n",
    "    print(text)\n",
    "\n",
    "# Détection de l'encodage et du séparateur du fichier csv :\n",
    "def detect_encoding_and_separator(filename):\n",
    "# Via la déclaration \"with\", le fichier est parcouru en mode binaire afin de détecter l'encodage : \n",
    "    with open(filename, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    encode = result['encoding']\n",
    "    print(f\"Le fichier est encodé en {encode}.\")\n",
    "# De nouveau avec \"with\", cette fois-ci le fichier est parcouru en mode text, afin d'examiner la structure même de la première ligne :\n",
    "    with open(filename, 'r', encoding=encode) as f:\n",
    "        separator = ';' if ';' in f.readline() else (',' if ',' in f.readline() else '\\t')\n",
    "        print(f\"Le fichier sépare les colonnes par le '{separator}'.\") if separator else print(\"Le séparateur n'a pas pu être détecté.\")\n",
    "        return encode, separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.1.3 Chargement des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des feuilles de données\n",
    "web = pd.read_excel(\"../feuilles_de_données/web.xlsx\") # Importation du fichier web.xlsx\n",
    "stock = pd.read_excel(\"../feuilles_de_données/erp.xlsx\") # Importation du fichier erp.xlsx\n",
    "link = pd.read_excel(\"../feuilles_de_données/liaison.xlsx\") # importation du fichier liaison.xlsx\n",
    "# Renommage de la table link :\n",
    "link_sku = link.rename(columns={'id_web': 'sku'})\n",
    "# importation du fichier caracteristiques_vins.csv :\n",
    "csv_file = \"../feuilles_de_données/caracteristiques_vins.csv\"\n",
    "encode, separator = detect_encoding_and_separator(csv_file)\n",
    "caracs = pd.read_csv(csv_file, sep=separator, encoding=encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: RGB(51,165,182);\" >\n",
    " <h2 style=\"margin: auto; padding: 20px; color:#fff; \">Etape 2 - Analyse exploratoire des fichiers</h2>\n",
    " </div><br>\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.1 - Analyse exploratoire du fichier erp.xlsx</h3>\n",
    " </div>\n",
    " Nous ne connaissons pas les différentes données des quatre tables. Nous allons donc commencer l'analyse par la découverte des colonnes, les types, le nombre de lignes.\n",
    " Il faut aussi vérifier les lignes et valeurs non nulles, ou dupliquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les infos d'ERP :\n",
    "get_table_info(stock, 'ERP')\n",
    "#Afficher les valeurs distinctes de la colonne stock_status\n",
    "analyze_dataframe(stock, 'ERP', ['product_id'], 'stock_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#À quelle(s) autre(s) colonne(s) sont-elles liées ?\n",
    "stock_corr = stock.drop('stock_status', axis=1)\n",
    "print(f\"Voici la correlation entre les différentes colonnes :\\n\")\n",
    "display(stock_corr.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  La correlation la plus proche de 1 est celle entre stock_quantity et stock_status_num, qui est stock_status converti en booléen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une colonne \"stock_status_2\n",
    "#La valeur de cette deuxième colonne sera fonction de la valeur dans la colonne \"stock_quantity\"\n",
    "#si la valeur de la colonne \"stock_quantity\" est nulle renseigner \"outofstock\" sinon mettre \"instock\"\n",
    "stock['stock_status_num'] = stock.stock_quantity.apply(lambda x: 'instock' if x > 0 else 'outofstock')\n",
    "stock = stock[[\"product_id\", \"onsale_web\", \"price\", \"stock_quantity\", \"stock_status\", \"stock_status_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vérifions que les 2 colonnes sont identiques:\n",
    "#Les 2 colonnes sont strictement identiques si les valeurs de chaque ligne sont strictement identiques 2 à 2\n",
    "#La comparaison de 2 colonnes peut se réaliser simplement avec l'instruction ci-dessous:\n",
    "print(f\"La conformité entre les deux colonnes est {stock.stock_status.equals(stock.stock_status_num)}\")\n",
    "\n",
    "#Le résultat est l'affichage de True ou False pour chacune des lignes du dataset\n",
    "#C'est un bon début, mais difficile à exploiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mais il est possible de synthétiser ce résultat en effectuant la somme de cette colonne:\n",
    "#True vaut 1 et False 0\n",
    "#Nous devrions obtenir la somme de 824 qui correspond au nombre de lignes dans ce dataset\n",
    "print(f\"la somme des valeurs conforme entre les deux colonnes s'élève à {sum(stock.stock_status == stock.stock_status_num)}, sur les {stock.stock_status_num.count()} lignes présentes. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si les colonnes ne sont absolument pas identiques ligne à ligne alors identifier la ligne en écart\n",
    "##Dans ce cas je vous ce lien pour apprendre à réaliser des filtres dans Pandas:\n",
    "##https://bitbucket.org/hrojas/learn-pandas/src/master/\n",
    "##Lesson 3\n",
    "display(stock.query('stock_status != stock_status_num').loc[:, ['stock_status', 'stock_status_num']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corriger la ou les données incohérentes\n",
    "stock.loc[443, 'stock_status'] = 'outofstock' if stock.loc[443, 'stock_quantity'] == 0 else 'instock'\n",
    "print(f\"La conformité entre les deux colonnes est {stock.stock_status.equals(stock.stock_status_num)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.1.1 - Analyse exploratoire de chaque variable du fichier erp.xlsx</h3>\n",
    " </div><br>\n",
    "\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.1.1.1 - Analyse de la variable PRIX</h3>\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vérification des prix: Y a t-il des prix non renseignés, négatif ou nul?\n",
    "filter_stock = stock.price.where(stock.price <= 0, np.nan).notnull()\n",
    "#Afficher le ou les prix non renseignés dans la colonne \"price\"\n",
    "print(f\"Nombres d'article avec un prix non renseignés: {filter_stock.sum()} article.\")\n",
    "# Afficher le prix minimum de la colonne \"price\"\n",
    "print(f\"Le prix le plus faible proposé est : {stock['price'].min()}€.\")\n",
    "# Afficher le prix maximum de la colonne \"price\"\n",
    "print(f\"Le prix le plus élevé proposé est : {stock['price'].max()}€.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.1.1.2 - Analyse de la variable STOCK</h3>\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la quantité minimum de la colonne \"stock_quantity\"\n",
    "print(f\"La quantité minimum de stock parmi les différents produits se dénombre à {stock['stock_quantity'].min()} articles.\")\n",
    "# Afficher la quantité maximum de la colonne \"stock_quantity\"\n",
    "print(f\"La quantité maximum de stock parmi les différents produit se dénombre à {stock['stock_quantity'].max()} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.1.1.3 - Analyse de la variable ONSALE_WEB</h3>\n",
    " </div>\n",
    "\n",
    " Vérification de la colonne onsale_web et des valeurs qu'elle contient? Que signifient-elles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(stock[['onsale_web','product_id']].groupby(['onsale_web']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  La colonne **[onsale_web]** représentée par un booléen (0 ou 1), signifie la présence ou non du produit sur le site web.\n",
    " > Cela nous est utile afin de ne garder que les produit du stock présents sur le site, et par conséquent, de se délester de cette colonne.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Les produits figurants sur le site web se dénombrent à {stock.onsale_web.sum()}, parmi les {stock.onsale_web.count()} articles enregistrés.\\n{len(stock) - stock.onsale_web.sum()} articles ne sont donc pas mis en ventes\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Quelles sont les colonnes à conserver selon vous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_onsaleTrue=stock[stock.onsale_web == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Uniquement les colonnes :\n",
    "  >    - [\"product_id\"] ;\n",
    "  >    - [\"price\"] ;\n",
    "  >    - [\"stock_quantity\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprimer les colonnes comportant le libellé \"stock_status\"\n",
    "#Cette colonne est redondante avec la colonne \"stock_quantity\". Dans notre projet cette information n'est pas utile.\n",
    "stock_lighter = stock_onsaleTrue.drop(['stock_status', 'stock_status_num', 'onsale_web', 'stock_status_num'], axis=1)\n",
    "stock_lighter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Résumé d'_ERP :_\n",
    "  - 3 colonnes et 717 lignes dont l'index n'est pas réinitialisé (allant de 0 à 824);\n",
    "  - Une clé \"primaire\" **[product_id]**, est un **int64**. Aucune donnée de ses n'est dupliquée, ni nulle ;\n",
    "  - Aucune des données d'_**ERP**_ ne semble transgresser le type de donnée attribuée à la colonne.\n",
    "  - l’intérêt est d'analyser les données selon les ventes sur le site internet. Dans la colonne onsale_web, 108 produits ne semblent pas figurer dans le catalogue. Donc nous les écartons, puis la colonne avec.\n",
    "  - La colonne **[stock_status]**, semble redondante avec **[stock_quantity]**. Nous pourrons l'abandonner.\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.2 - Analyse exploratoire du fichier web.xlsx</h3>\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les infos de Web :\n",
    "get_table_info(web, 'Web')\n",
    "analyze_dataframe(web, 'Web', ['sku'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Selon vous, quelles sont les colonnes à conserver ?\n",
    "  > [\"post_title\", \"total_sales\",\"post_date_gmt\"] de manière définitive, et [\"post_name\", \"sku\", \"post_status\", \"post_type\", \"post_mime_type\"] de manière temporaire<br>\n",
    "\n",
    " Si vous avez défini des colonnes à supprimer, effectuer l'opération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_lighterCol = web.drop(['virtual', 'downloadable', 'rating_count', 'average_rating', 'tax_status', 'tax_class', 'post_author', 'post_date', 'post_content', 'post_excerpt', 'comment_status', 'ping_status', 'post_password', 'post_modified', 'post_modified_gmt', 'post_content_filtered', 'post_parent', 'guid', 'menu_order', 'comment_count'], axis = 1)\n",
    "print(web_lighterCol.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Visualisation des valeurs de la colonne sku\n",
    "Quelles sont les valeurs qui ne semblent pas respecter la règle de codification?\n",
    "display(web_lighter.sku.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_lighter_wrong_sku = web_lighterCol['sku'].apply(lambda x: not str(x).isdigit())\n",
    "wrong_sku = web_lighterCol.loc[web_lighter_wrong_sku, 'sku'].unique()\n",
    "print(f\"Les codes ne respectant pas la règle de codification sont {wrong_sku}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les codes articles identifiés, réalisez une analyse et définissez l'action à entreprendre  \n",
    ">Créer un sous-ensemble avec les lignes invalides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = web_lighterCol.loc[web_lighter_wrong_sku]\n",
    "# Marquer les lignes dupliquées\n",
    "invalid_rows['duplicated'] = invalid_rows.duplicated(keep=False)\n",
    "# Remplacer les valeurs NaN par une valeur spécifique\n",
    "invalid_rows = invalid_rows.fillna('missing')\n",
    "# Compter le nombre de lignes dupliquées pour chaque groupe\n",
    "invalid_rows['count_even_row'] = invalid_rows.groupby(invalid_rows.columns.tolist())['duplicated'].transform('sum')\n",
    "# Supprimer les lignes dupliquées\n",
    "invalid_rows = invalid_rows.drop_duplicates()\n",
    "# Afficher les résultats\n",
    "display(invalid_rows.sort_values(by='sku'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  La clé pour chaque ligne est-elle uniques? ou autrement dit, y a-t-il des doublons?\n",
    "  > La plupart des doublons sont des valeurs nulles, mais 2 doublons sont discernables, celles des 2 codes sku non réglementaires, avec un point commun :\n",
    "  > - La valeur product et la valeur attachment de la colonne 'post_mime_type', cette dernière valeur accompagnée d'une image jpeg.\n",
    "  Nous pouvons donc assumer que la duplication de toutes les valeurs de 'sku' non nulles, proviennent des pièces jointes jpeg.\n",
    "  Par contre, dans les autres colonnes, il se trouve 2 valeurs de plus non nulles, il faudrait donc les inspecter.\n",
    "  Identifier les lignes sans code articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_sku_nul = web_lighterCol['sku'].isnull()\n",
    "print(f\"Il se trouve {sum(web_sku_nul)} codes article nulle\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Les lignes sans code article semble être toutes non renseignés\n",
    "     Pour s'en assurer réaliser les étapes suivantes:\n",
    " 1 - Créer un dataframe avec uniquement les lignes sans code article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_no_sku = web_lighterCol[web_sku_nul]\n",
    "# 2 - utiliser la fonction stock_web.info() sur ce nouveau dataframe pour observer le nombre de valeur renseignée dans chacune des colonnes\n",
    "web_no_sku.info()\n",
    "# 3 - Que constatez-vous?\n",
    "web_filter_notna = (web_no_sku.drop('sku', axis=1).notna().any(axis=1))\n",
    "web_notna = web_no_sku[web_filter_notna]\n",
    "display(web_notna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dans chaque colonnes se trouve 2 valeurs, hormis pour sku et post_mime_type.<br>\n",
    " Nous avons donc aucune utilité à garder ces lignes dans la table actuelle.<br>\n",
    " Par contre, un nom permet d'identifier l'article, et donc de pouvoir réfléchir à le réintroduire ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_lighter_na = web_lighterCol.dropna(subset=['sku'])\n",
    "print(f\"Il se trouve {sum(web_lighter_na['sku'].isnull())} codes article nulle\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nous pouvons maintenant vérifier une dernière fois les infos du tableau afin d'entreprendre les actions finales de la table Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_lighter_na.info()\n",
    "print(f'''Le résultat de la somme de lignes dans sku, soit {sum(web_lighter_na.sku.value_counts())}, soustrait par la somme de lignes dans post_mime_type, soit {sum(web_lighter_na.post_mime_type.value_counts())}, \n",
    "est égale à {(web_result := sum(web_lighter_na.sku.value_counts())-sum(web_lighter_na.post_mime_type.value_counts()))}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nous pouvons donc clore le nettoyage de la table web, par la suppression des lignes contenant la valeur 'attachment', puis retirer les colonnes superflues de la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_lighter_clean = web_lighter_na.drop(web_lighter_na[web_lighter_na['post_type'] == 'attachment'].index)\n",
    "analyze_dataframe(web_lighter_clean, 'Web', ['sku'])\n",
    "\n",
    "web_lighter_clean[web_lighter_clean.sku.duplicated()].info()\n",
    "web_lighter_clean.sort_values(by=['sku', 'post_type'])\n",
    "\n",
    "web_almost_light = web_lighter_clean.drop(['post_type', 'post_mime_type', 'post_status'], axis = 1)\n",
    "web_almost_light.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Résumé de _Web :_\n",
    "  - 5 colonnes et 716 lignes dont l'index n'est pas réinitialisé (allant de 0 à 798) ;\n",
    "  - Une clé \"primaire\" **[sku]** est un **object** ;\n",
    "  - Parmi les 28 colonnes, 4 colonnes ne contiennent aucune données, **[tax_class]**, **[post_content]**, **[post_password]**, **[post_content_filtered]**. Ces colonnes sont donc impertinentes.\n",
    "  - De plus, l'objectif étant l'analyse du chiffre d'affaire, seule la colonne indiquant le nombre de ventes, **[total_sales]**, serait pertinente.\n",
    "         Par contre, il faut pouvoir identifier chacun des articles, donc garder les colonnes de code article, **[sku]**, et du titre du produit, **[post_title]**, serait pertinent.\n",
    "         Ce n'est pas demandé, mais peut-être que la date selon le GMT, **[post_date_gmt]**, ne serait pas impertinent si il fallait comparer le nombre total de ventes, selon la date de publication.\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.3 - Analyse exploratoire du fichier liaison.xlsx</h3>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension du dataset\n",
    "#Nombre d'observations\n",
    "#Nombre de caractéristiques\n",
    "#Consulter le nombre de colonnes\n",
    "#La nature des données dans chacune des colonnes\n",
    "#Le nombre de valeurs présentes dans chacune des colonnes\n",
    "#Les valeurs de la colonne \"product_id\" sont elles toutes uniques?\n",
    "#Les valeurs de la colonne \"id_web\" sont-elles toutes uniques?\n",
    "analyze_dataframe(link_sku, \"Liaison\", ['product_id', 'sku'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avons-nous des articles sans correspondances?\n",
    "sum_na = link_sku['sku'].isna().sum()\n",
    "print(f\"Il se trouve {sum_na} articles sans correspondance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Résumé de _Link :_\n",
    "  - 2 colonnes et 825 lignes ;\n",
    "  - Une clé \"primaire\" **[product_id]** est un **int64** ;\n",
    "  - Aucune donnée de **[product_id]** n'est dupliquée, ni nulle ;\n",
    "  - Aucune des données de _**LINK**_ ne semble transgresser le type de donnée.\n",
    "  - Sa seconde colonne se nomme **[id_web]**, mais doit se renommer **[sku]**, c'est donc une feuille qui comme son nom l'indique permet de lier _**WEB**_, et _**ERP**_ ensemble.\n",
    "\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">2.4 - Analyse exploratoire du fichier caracteristiques_vins.xlsx</h3>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des infos de Caractéristiques_vins :\n",
    "get_table_info(caracs, \"Caractéristiques_Vins\")\n",
    "analyze_dataframe(caracs, \"Caractéristiques_Vins\", ['post_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quels sont les produits avec des informations manquantes?\n",
    "caracs_nul = caracs.isnull().any(axis=1)\n",
    "caracs_nan = caracs[caracs_nul]\n",
    "display(caracs_nan)\n",
    "print(caracs_nan.to_string(max_colwidth=39))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Est-il possible de corriger les données manquantes?\n",
    "  > A la main oui, en recherchant sur internet par nom de produit, car aucune donnée de produit n'est manquante.\n",
    "  Par contre, les colonnes contenant des valeurs manquantes, ne sont pas utiles au projet, et pourraient être supprimées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracs_lighter = caracs.drop([\"poids\", \"Région\", \"Domaine\", \"Appellation\", \"Couleur\", \"Cépage\", \"Millésime\", \"Garde\", \"Contenance\", \"Degré d'alcool\", \"Température dégustation\", \"Alliance mets\"], axis=1)\n",
    "caracs_lighter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Résumé de _Caractéristiques_Vins :_\n",
    "  - 13 colonnes et 611 lignes ;\n",
    "  - Une clé \"primaire\" **[post_name]** est un **object** ;\n",
    "  - Aucune donnée de **[post_name]** n'est dupliquée, ni nulle ;\n",
    "  - Aucune des données de _**CARACS**_ ne semble transgresser le type de donnée.\n",
    "  - La colonne **[post_name]**, comporte le même nom qu'une des colonnes de la table WEB, et comporte, avec plus de lisibilité, les mêmes données que la colonne **[post_title]**.\n",
    "  - Sur la même base que dans le paragraphe de _**WEB**_, seule la colonne post_name est pertinente. Il ne serait pas utile de lier ces deux tables intrinsèquement.\n",
    "        Cependant elle réfère l'une partie du catalogue, cette information pourrait se révéler utile aussi.\n",
    "\n",
    " <div style=\"background-color: RGB(51,165,182);\" >\n",
    " <h2 style=\"margin: auto; padding: 20px; color:#fff; \">Etape 3 - Jonction des fichiers</h2>\n",
    " </div>\n",
    "\n",
    "  #### 3.0.5 - Transversalité des feuilles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_link_occur = (stock.columns).intersection(link_sku.columns)\n",
    "print(f\"Colonne occurrente des tables ERP et Liaison : {stock_link_occur}\")\n",
    "print(separe)\n",
    "## La table link et web :\n",
    "web_link_occur = (web.columns).intersection(link_sku.columns)\n",
    "print(f\"Colonne occurrente des tables Web et Liaison : {web_link_occur}\")\n",
    "print(separe)\n",
    "## La table web et caracs :\n",
    "web_caracs_occur = (web.columns).intersection(caracs.columns)\n",
    "print(f\"Colonne occurrente des tables Web et Caractéristiques_vins : {web_caracs_occur}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Les feuilles ne possédant pas de colonnes communes :\n",
    "    - Les feuilles _**ERP**_ et _**Web**_ ne possèdent aucune colonne communes. _**Liaison**_ permettra de faire le pont entre ces deux feuilles.\n",
    "        - Liaison comporte la clé étrangère de _**ERP**_, **[product_id]**, et de _**Web**_, **[id_web]** devenant **[sku]**.\n",
    "    - La feuille _**Caractéristiques_vins**_ n'a en commun aucune colonne avec les feuilles _**Liaison**_ et _**ERP**_.\n",
    "        - _**Web**_ fera office de pont, comportant la clé étrangère **[post_name]**, en plus d'être liée à ERP grâce à la table _**Liaison**_.\n",
    "\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">Etape 3.1 - Jonction du fichier stock et df_liaison</h3>\n",
    " </div>\n",
    "\n",
    " Fusion des fichiers stock_web_erp et stock_web_liaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_link_Full = pd.merge(stock_lighter, link_sku, on='product_id', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y a t-il des lignes ne \"matchant\" entre les 2 fichiers?\n",
    "analyze_dataframe(stock_link_Full, 'ERP_Liaison',['product_id'])\n",
    "stock_link_Full._merge.value_counts()\n",
    "\n",
    "stock_link_Full.drop(stock_link_Full[stock_link_Full['_merge'] == 'right_only'].index, inplace=True)\n",
    "stock_link_Full.drop(\"_merge\", axis=1, inplace = True)\n",
    "stock_link_Full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">Etape 3.2 - Jonction du fichier df_merge et df_web</h3>\n",
    " </div>\n",
    "\n",
    "Fusionnez les datasets df_merge et df_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_join = pd.merge(stock_link_Full, web_almost_light, on='sku',  how = 'outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avons-nous des lignes sans correspondances?\n",
    "analyze_dataframe(middle_join, 'ERP_Liaison_Web', ['product_id'])\n",
    "join_na = middle_join['sku'].isna().sum()\n",
    "print(f\"Il se trouve {join_na} articles sans correspondance.\")\n",
    "mask = middle_join['sku'].isna()\n",
    "result = middle_join.loc[mask]\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  N'ayant même pas de code article, il ne sont dont pas vendable, le nombre total de ventes est nul, il n'est donc pas utile de les intégrer dans l'analyse du chiffre d'affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_null = middle_join[middle_join['sku'].isna()].index\n",
    "middle_join_clean = middle_join.drop(drop_null)\n",
    "data_middle = middle_join_clean.drop('_merge', axis=1)\n",
    "print(data_middle.info())\n",
    "display(data_middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">Etape 3.3 - Jonction du fichier df_merge et df_caracteristiques</h3>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fusion de la table df_merge et df_caracteristiques \n",
    "data_full = pd.merge(data_middle, caracs_lighter, on='post_name',  how = 'outer')\n",
    "analyze_dataframe(data_full, 'ERP_Liaison_Web_Caractéristiques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge = data_full.drop(['post_name', 'product_id', 'sku', 'post_date_gmt'], axis = 1)\n",
    "df = data_merge[['post_title', 'stock_quantity', 'price', 'total_sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: RGB(51,165,182);\" >\n",
    " <h2 style=\"margin: auto; padding: 20px; color:#fff; \">Etape 4 - Analyse univarié des prix</h2>\n",
    " </div>\n",
    "\n",
    " Nous allons tirer des graphiques contenant des valeurs de colonnes telles que :\n",
    "\n",
    " - Le revenu par articles, soit l'**[income_items]** :\n",
    "   - Produit du prix unitaire **[price]** par l'écoulement des stocks **[total_sales]** ;\n",
    " - Le nombre d'écart-type, soit le Z-score du prix **[zscore_per_price]** :\n",
    "   - Quotient de la division du résultat, de la soustraction entre le prix unitaire **[price]** par la moyenne des prix unitaire **[price]**, par l'écart-type du prix unitaire **[price]** ;\n",
    "\n",
    " Afin d'obtenir les résultats les plus cohérents et servant de prévisions, il faudra mettre en évidence les valeurs aberrantes (outliers), grâce à :\n",
    "\n",
    " - l'écart-type ;\n",
    " - l'interquartile ;\n",
    " - le zscore ;\n",
    " #### Préparation des nouvelles colonnes, de leurs appels et d'équations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df.price\n",
    "total_sales = df.total_sales\n",
    "# Utilisation de la fonction describe de Pandas pour l'etude des mesures de dispersions\n",
    "outlier = price.describe()\n",
    "# Initialisation des outliers\n",
    "# Premier quartile\n",
    "q1 = outlier['25%']\n",
    "# Troisième quartile\n",
    "q3 = outlier['75%']\n",
    "# Écart interquartile\n",
    "iqr = q3 - q1\n",
    "# Calcul des valeurs aberrantes\n",
    "outlier_threshold = 1.5 * iqr\n",
    "# Création des limites, respectivement supérieur, et inférieur\n",
    "limit_higher = q3 + outlier_threshold\n",
    "limit_lower = q1 - outlier_threshold\n",
    "# Stockage des valeurs aberrantes \n",
    "outliers = price[(price < limit_lower) | (price > limit_higher)]\n",
    "# Totalise le décompte des valeurs aberrantes\n",
    "sum_outliers = len(outliers)\n",
    "# Calcul de la proportion des valeurs aberrantes\n",
    "prop_outliers = sum_outliers / len(price)\n",
    "# Calculer la moyenne du prix\n",
    "mean = sum(price) / len(price)\n",
    "mean_higher = price > mean\n",
    "# Calculer l'écart-type du prix\n",
    "std = math.sqrt(sum((price - mean)**2) / len(price))\n",
    "\n",
    "# Calculer le prix médian\n",
    "def median(column):\n",
    "    n = len(column)\n",
    "    sort_col = sorted(column)\n",
    "    if n % 2 == 0:\n",
    "        # Cas pair\n",
    "        mid = n // 2\n",
    "        return (sort_col[mid - 1] + sort_col[mid]) / 2\n",
    "    else:\n",
    "        # Cas impair\n",
    "        mid = (n - 1) // 2\n",
    "        return sort_col[mid]\n",
    "\n",
    "mediane = median(price)\n",
    "# Création des colonnes\n",
    "    # Créez une colonne calculant le CA par article\n",
    "df[\"income_items\"] = price*total_sales\n",
    "income_items = df.income_items\n",
    "# Calcul du chiffre d'affaire\n",
    "turnover = sum(income_items)\n",
    "\n",
    "df[\"zscore_price\"] = (price - mean) / std\n",
    "zscore = df.zscore_price\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "df[['post_title', 'price', 'total_sales', 'income_items', 'stock_quantity', 'zscore_price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">Etape 4.1 - Exploration par la visualisation de données</h3>\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une Boite à moustache de la répartition des prix grâce à Pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = df.boxplot(column='price',\n",
    "                vert=False,\n",
    "                flierprops=dict(markerfacecolor='orange',\n",
    "                                markeredgecolor='red'))\n",
    "ax.set(title='Répartition des prix', xlabel='Prix')\n",
    "ax.axvline(x=round(limit_higher,2), color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Le boxplot est représenté par le carré et ses \"moustaches\", indiquant à leur extrémité, les seuils de valeurs aberrantes.<br>\n",
    " Les \"moustaches\" représentent les valeurs dites \"cohérentes, rationnelles\", tandis que les points oranges représentent les valeurs aberrantes.<br>\n",
    " La ligne pointillée rouge indique visiblement le seuil de ces dernières.<br>\n",
    " La ligne verte représente la médiane des prix. Le coté gauche et droit du carré sont respectivement le quantile 1 et 3.<br>\n",
    " Ce qui se trouve à gauche du quantile 1 représente les 25% d'articles aux prix les plus bas, et à droite du quantile 3 sont représentés les 25% d'articles aux prix les plus hauts.<br>\n",
    " Les boxplot et points rouges illustrent le prix, indiqué par l'axe X, de chaque produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autre méthode avec plotly express\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Violin(x=df['price'], box_visible=True, points='outliers', name='Price', orientation='h',\n",
    "                         marker=dict(color='orange'),\n",
    "                         line=dict(color='red')))\n",
    "fig.update_layout(title='Répartition des prix', xaxis_title='Prix', yaxis_title='Volume d\\'articles')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " J'ai choisi le violin plot pour visualiser les données autrement, sans dénaturer la pertinence des infos.<br>\n",
    " Cela représente le mélange d'un boxplot, comme vu précédemment, d'un violin plot, et d'un marker des outliers.<br>\n",
    " Ce graphique lie donc :\n",
    " - Répartition des prix ;\n",
    " - densité des valeurs ;\n",
    "\n",
    "  ## Etape 4.2 - Exploration par l'utilisation de méthodes statistique\n",
    "  ### Etape 4.2.1 - Identification par le Z-index\n",
    "  #### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de références\n",
    "print(f\"Il s'y cumule {round(outlier[0],1)} articles\")\n",
    "# Affichage de la moyenne\n",
    "print(f\"Le prix moyen d'une bouteille est de {round(outlier[1],1)}€\")\n",
    "# Affichage l'écart-type\n",
    "print(f\"l'écart type est de {round(outlier[2],1)}€\")\n",
    "# Affichage la médiane\n",
    "print(f\"La médiane est de {round(outlier[5],1)}€\")\n",
    "# Définissez un seuil pour les articles \"outliers\" en prix\n",
    "print(f\"Les valeurs aberrantes se situent en dehors de {round(limit_lower, 2)}€ et {round(limit_higher,2)}€\")\n",
    "# Définissez le nombre d'articles et la proportion de l'ensemble du catalogue \"outliers\"\n",
    "print(f'''Il y a {sum_outliers} références produit dans la dataset qui sont considérées comme des valeurs aberrantes.\n",
    "La proportion de valeurs aberrantes dans l'ensemble du catalogue est de {prop_outliers:.2%}.''')\n",
    "\n",
    "\n",
    "# Compter le nombre d'outliers supérieurs à la limite supérieure\n",
    "# Afficher les résultats\n",
    "print(f\"Nombre d'outliers supérieurs à la limite supérieure: {sum_outliers}\")\n",
    "print(f\"Proportion d'outliers supérieurs à la limite supérieure: {prop_outliers:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Selon vous, ces outliers sont-ils justifiés ? Comment le démontrer si cela est possible ?<br>\n",
    " > Pour déterminer s'il est justifié d'inclure les outliers dans notre analyse, une approche consiste à évaluer si ces valeurs extrêmes sont cohérentes avec la nature de nos données.<br>\n",
    " Plusieurs méthodes peuvent être utilisées pour cela, telles que l'analyse de la distribution des données, l'examen de la provenance des outliers ou encore l'étude des valeurs adjacentes aux outliers.<br>\n",
    " Dans le cas spécifique de notre étude sur les prix des bouteilles de vin, il convient de noter que la limite inférieure ne peut être inférieure au prix minimum d'une bouteille, puisqu'un prix de vente est toujours positif et supérieur à 0.<br>\n",
    "\n",
    " Toutefois, si l'outlier inférieur est inférieur, proche ou égal à 0€, il est possible qu'il soit justifié dans la mesure où cela pourrait correspondre à une promotion ou à une erreur de saisie.<br>\n",
    " D'un autre côté, si l'outlier supérieur semble incohérent, il peut être utile de comparer les prix avec ceux proposés par la concurrence pour le vin le plus cher de Bottleneck.<br>\n",
    " En fin de compte, la décision d'inclure ou d'exclure les outliers de notre analyse dépendra du contexte de notre étude et de l'objectif de notre analyse.<br>\n",
    "\n",
    " Si le outlier supérieur semble incohérent, nous pourrions analyser chez la concurrence le prix donné au vin le plus cher de Bottleneck :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 47)\n",
    "print(f\"Voici le vin le plus onéreux de Bottleneck :\\n {df[df['price'] == price.max()].to_string(index=False)}\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nous pouvons trouver le moins cher au prix de 299,95€, mais 2 sites le proposent à 350-400€, et 2 autres entre 828 et 961,28€.\n",
    "\n",
    " Quel est le seuil prix dont z-score est supérieur à 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[zscore > 3].sort_values(by='zscore_price', ascending=False))\n",
    "print(f\"Il se trouve {sum(zscore > 3)} articles possédant un zscore supérieur à 3.\")\n",
    "# Et pour un seuil supérieur à 2\n",
    "print(f\"Il se trouve {sum(zscore > 2)} articles possédant un zscore supérieur à 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Etape 5 - Analyse univariée du CA et des quantités vendues\n",
    " ### Etape 5.1 - Analyse des ventes en CA\n",
    " #### Etape 5.1.1 Calculer le CA du site web\n",
    " Calculez la somme de la colonne \"ca_par_article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le chiffre d'affaire effectué sur le site est de {turnover}€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ce résultat correspond au chiffre d'affaire du site web\n",
    "\n",
    " #### Etape 5.1.2 - Palmarès des articles en CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer le tri dans l'ordre décroissant du CA du dataset stock_web_merge\n",
    "sorted_income_items = df.sort_values(by='income_items', ascending=False)\n",
    "# Réinitialiser l'index du dataset par un reset_index\n",
    "sorted_income_items = sorted_income_items.reset_index(drop=True)\n",
    "# Afficher les 20 premier articles en CA\n",
    "display(sorted_income_items.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Graphique en barre des 20 premiers articles en quantité de vente, avec plotly express :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_top = sorted_income_items\n",
    "df_graph_top20 = df_graph_top.head(20)\n",
    "graph = df_graph_top20.plot(kind=\"bar\", x=\"post_title\", y=\"income_items\",figsize=(10,6))\n",
    "graph.set(xlabel=\"Article\", ylabel=\"Top des articles vendus\")\n",
    "graph.set_title('Les 20 articles les plus vendus')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nous pouvons constater dans ce graphique deux articles se distinguant par un revenu bien supérieur aux autres vins.<br>\n",
    " Le premier article en terme de vente est 2 fois supérieur au troisième article le plus vendu.\n",
    " #### Etape 5.1.3 - Calculer le 20 / 80 en CA\n",
    " Créer une colonne calculant la part du CA de la ligne dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seller = df.copy()\n",
    "df_seller['turnover_part'] = income_items / turnover * 100\n",
    "df_sort_seller = df_seller.sort_values(by='turnover_part', ascending=False)\n",
    "df_sort_seller = df_sort_seller.reset_index(drop=True)\n",
    "# Créer une colonne réalisant la somme cumulative de la colonne précédemment créée\n",
    "df_sort_seller['turnover_cumul'] = df_sort_seller['turnover_part'].cumsum()\n",
    "# Grâce au deux colonnes créées précédemment, calculer le nombre d'articles représentant 80% du CA\n",
    "seller_80pct = df_sort_seller[df_sort_seller['turnover_cumul'] >= 80].index[0]\n",
    "sum_80pct = seller_80pct + 1\n",
    "print(f\"Le nombre d'articles représentant 80% du CA est de {sum_80pct} articles\")\n",
    "# Afficher la proportion que représentent ce groupe d'articles dans le catalogue entier du site web\n",
    "total_articles = len(df_sort_seller)\n",
    "proportion_80pct = sum_80pct / total_articles\n",
    "print(f\"La proportion d'articles représentant plus de 80% du CA est de {proportion_80pct:.2%}\")\n",
    "print(df_sort_seller.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">Etape 5.2 - Analyse des ventes en Quantités</h3>\n",
    " </div>\n",
    "\n",
    " #### Palmares des articles en quantité\n",
    "\n",
    " Effectuer le tri dans l'ordre décroissant de quantités vendues du dataset stock_web_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_qtt_items = df.sort_values(by='stock_quantity', ascending=False)\n",
    "# Réinitialiser l'index du dataset par un reset_index\n",
    "sorted_qtt_items\n",
    "sorted_qtt_items = sorted_qtt_items.reset_index(drop=True)\n",
    "# Afficher les 20 premier articles en quantité\n",
    "display(sorted_qtt_items.head(20))\n",
    "# Graphique en barre des 20 premiers articles avec plotly express\n",
    "df_graph_top = sorted_qtt_items\n",
    "df_graph_top20 = df_graph_top.head(20)\n",
    "graph = df_graph_top20.plot(\n",
    "    kind=\"bar\", x=\"post_title\", y=\"stock_quantity\", figsize=(10, 6))\n",
    "graph.set(xlabel=\"Article\", ylabel=\"Top des articles en stock\")\n",
    "graph.set_title('Les 20 premiers articles en stock')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Calculer le 20 / 80 en CA\n",
    "\n",
    " Créer une colonne calculant la part en quantité de la ligne dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumtotal_sales = df['total_sales'].sum()\n",
    "\n",
    "def calculate_sales_part(row):\n",
    "    return row['total_sales'] / sumtotal_sales * 100\n",
    "\n",
    "df['sales_part'] = df.apply(calculate_sales_part, axis=1)\n",
    "df_sort_qtt = df.sort_values(by='sales_part', ascending=False)\n",
    "df_sort_qtt = df_sort_qtt.reset_index(drop=True)\n",
    "# Créer une colonne réalisant la somme cumulative de la colonne précédemment créée\n",
    "df_sort_qtt['sales_cumul'] = df_sort_qtt.sales_part.cumsum()\n",
    "# Grâce au deux colonnes créées précédemment, calculer le nombre d'articles représentant 80% des ventes en quantité\n",
    "qtt_items80 = df_sort_qtt[df_sort_qtt.sales_cumul >= 80].index[0]\n",
    "sum_qtt80 = qtt_items80 + 1\n",
    "print(f\"Le nombre d'articles représentant 80% des ventes en quantité est de {sum_qtt80} articles\")\n",
    "# Calculer la proportion que représente ce groupe d'articles dans le catalogue entier du site web\n",
    "proportion_qtt80 = sum_qtt80 / total_articles\n",
    "# Afficher la proportion\n",
    "print(f\"Le groupe d'articles représentant 80% des ventes en quantité représente {proportion_qtt80:.2%} du catalogue entier du site web.\")\n",
    "\n",
    "print(df_sort_qtt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusion :\n",
    " Nous pouvons donc constater 2 résultats à partir de la proportion des articles représentant 80% des ventes en quantité, et le groupe d'articles représentant 80% du chiffre d'affaire :\n",
    " - Respectivement 18.35% et 21.15%.\n",
    " Ce qui serait bien de vérifier ? Quels sont les articles en communs parmi ces deux groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_100pct = df_sort_seller[df_sort_seller.turnover_cumul == 100].index[0]\n",
    "qtt_items100 = df_sort_qtt[df_sort_qtt.sales_cumul >= 100].index[0]\n",
    "\n",
    "df1 = df_sort_seller.iloc[:seller_100pct]\n",
    "df2 = df_sort_qtt.iloc[:qtt_items100]\n",
    "df_100pct = pd.merge(df1, df2, on='post_title', how='outer', indicator=True)\n",
    "\n",
    "# Supprimer les lignes où les colonnes 'turnover_cumul' et 'sales_cumul' sont égales à 0\n",
    "df_100pct = df_100pct[(df_100pct['turnover_cumul'] != 0) & (df_100pct['sales_cumul'] != 0)]\n",
    "\n",
    "# Réinitialiser l'index du dataframe\n",
    "df_100pct = df_100pct.reset_index(drop=True)\n",
    "df_100pct = df_100pct.rename(columns={'total_sales_x': 'total_sales', 'stock_quantity_x': 'stock_quantity', 'price_x': 'price', 'income_items_x': 'income_items', 'zscore_price_x': 'zscore_price'})\n",
    "\n",
    "# Regrouper les données par la colonne 'turnover_cumul' et trier les groupes par ordre croissant\n",
    "grouped = df_100pct.groupby('turnover_cumul').apply(lambda x: x.sort_values('turnover_cumul'))\n",
    "\n",
    "df_100pct_clean = df_100pct[['post_title', 'stock_quantity', 'price', 'total_sales', 'income_items', 'zscore_price', 'turnover_part', 'turnover_cumul', 'sales_part', 'sales_cumul', '_merge']]\n",
    "# Afficher les résultats\n",
    "print(df_100pct_clean.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " J'ai lié deux dataframes, dont le but est de retenir les noms de vins, se dénombrant à 386 vins représentant 100% de la vente en quantité, et 100% de la vente en CA. Je n'ai pu réglé l'erreur de la dernière ligne, représentant le 100ème pourcent.<br>\n",
    " cette ligne contient des pseudos valeurs nulles, liée au fait que ce vin n'est pas présent dans le dataframe des ventes en quantité. En tout les cas, ce tableau fusionné est le produit de tous les articles impactant le chiffre d'affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer ce dataframe, et les articles les plus en stock\n",
    "top_20 = sorted_qtt_items.head(20)\n",
    "# Créer une colonne booléenne pour indiquer si une ligne est présente dans les deux dataframes ou non\n",
    "df_100pct_clean['in_both'] = df_100pct_clean['stock_quantity'].isin(top_20['stock_quantity'])\n",
    "row = df_sort_seller.loc[df_sort_seller['post_title'] == 'Château Turcaud Bordeaux Rosé 2019']\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f'''Parmi les articles représentant la totalité du chiffre d\\'affaire, {len(df_100pct_clean) - sum(df_100pct_clean.in_both)} se retrouvent en rupture de stock.\n",
    "      Pour aller plus loin, parmi les {sum(df.stock_quantity)} en stock, {sum(df.stock_quantity) - sum(df_100pct_clean.in_both) - row['stock_quantity'].iloc[0]} ne sont pas vendus.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " J'ai donc mis en avant le nombre de bouteilles encore en stock, et j'ai récupéré la ligne problématique de mon tableau provenant de la fusion précédemment citée.\n",
    " <div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    " <h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">Etape 5.3 - Mettre à disposition la nouvelle table sur un fichier Excel</h3>\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre le dataset stock_web_merge sur un fichier Excel\n",
    "# Cette étape peut-être utile pour partager le résultat du dataset obtenu pour le partager avec les équipes.\n",
    "data_full.to_excel(\"../feuilles_de_données/stock_web_merge.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
